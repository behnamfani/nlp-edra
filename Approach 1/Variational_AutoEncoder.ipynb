{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, metrics, Model, losses\n",
        "import pandas as pd\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "import time"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HuQATS3A49y8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Libraries*** ðŸ‘†\n",
        "---\n",
        "# ***Methods and Classes*** ðŸ‘‡\n"
      ],
      "metadata": {
        "id": "PgHC8q4GEZ0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Samling\n",
        "class Sampling(layers.Layer):\n",
        "  '''\n",
        "  Sampling Layer: Sample z from the Probability Distribution of z_mean and z_log_var\n",
        "  '''\n",
        "  def call(self, z_mean, z_log_var):\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    epsilon = tf.random.normal(shape=(batch, dim))\n",
        "    z = z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "    return z"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TF2lmw9i5De7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title VAE Model\n",
        "class VAE(keras.Model):\n",
        "\n",
        "  def __init__(self, input_dim:int, hidden_dim:int, latent_dim:int, **kwargs):\n",
        "    '''\n",
        "    Define the model structure and it's properties\n",
        "    '''\n",
        "    super().__init__(**kwargs)\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.latent_dim = latent_dim\n",
        "    self.encoder = self.Encoder()\n",
        "    self.decoder = self.Decoder()\n",
        "    self.total_loss = metrics.Mean(name=\"total_loss\")\n",
        "    self.reconstruction_loss = metrics.Mean(name=\"reconstruction_loss\")\n",
        "    self.kl_loss = metrics.Mean(name=\"kl_loss\")\n",
        "    self.epoch = 0\n",
        "    self.x = tf.Variable(10, trainable=False, dtype=float)\n",
        "\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    '''\n",
        "    Loss metrics\n",
        "    '''\n",
        "    return [self.total_loss, self.reconstruction_loss, self.kl_loss,]\n",
        "\n",
        "\n",
        "  def Encoder(self)->Model:\n",
        "    '''\n",
        "    Encoder model to transform the input to the latent space (compress)\n",
        "    '''\n",
        "    encoder_inputs = keras.Input(shape=(self.input_dim,))\n",
        "    x = layers.Dense(self.hidden_dim, activation=\"relu\")(encoder_inputs)\n",
        "    z_mean = layers.Dense(self.latent_dim, name=\"z_mean\")(x)\n",
        "    z_log_var = layers.Dense(self.latent_dim, name=\"z_log_var\")(x)\n",
        "    z = Sampling()(z_mean, z_log_var)\n",
        "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "    encoder.summary()\n",
        "    keras.utils.plot_model(encoder, show_shapes=True, to_file='Encoder.png')\n",
        "    return encoder\n",
        "\n",
        "\n",
        "  def Decoder(self)->Model:\n",
        "    '''\n",
        "    Decoder model to reconstruct the input from the latent vector  (decompress)\n",
        "    '''\n",
        "    latent_inputs = keras.Input(shape=(self.latent_dim,))\n",
        "    x = layers.Dense(self.hidden_dim, activation=\"relu\")(latent_inputs)\n",
        "    decoder_outputs = layers.Dense(self.input_dim, activation=\"relu\")(x)\n",
        "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
        "    decoder.summary()\n",
        "    keras.utils.plot_model(decoder, show_shapes=True, to_file='Decoder.png')\n",
        "    return decoder\n",
        "\n",
        "\n",
        "  def Loss(self, input: tf.Tensor, output: tf.Tensor, z_mean: tf.Tensor, z_log_var: tf.is_tensor, beta: int)->list:\n",
        "    '''\n",
        "    The Loss fuction to calculate the loss of VEA (Reconstruction_loss+KL_loss)\n",
        "    '''\n",
        "    Reconstruction_loss = tf.reduce_mean((input-output)**2)\n",
        "    KL_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "    KL_loss = tf.reduce_mean(tf.reduce_sum(KL_loss))\n",
        "    return [Reconstruction_loss + beta*KL_loss, Reconstruction_loss, KL_loss]\n",
        "\n",
        "\n",
        "  def train_step(self, input: tf.Tensor, beta:int=0)->dict:\n",
        "    '''\n",
        "    Calculate the output of the model and the errors.\n",
        "    Update the model's weights by Backpropagating the error\n",
        "    As the loss of KL_divergence is bigger than Reconstruction, first warmup the model by setting the beta=0\n",
        "    '''\n",
        "    with tf.GradientTape() as tape:\n",
        "      z_mean, z_log_var, z = self.encoder(input)\n",
        "      output = self.decoder(z)\n",
        "      Total_loss, Reconstruction_loss, KL_loss = self.Loss(input, output, z_mean, z_log_var, beta)\n",
        "      grads = tape.gradient(Total_loss, self.trainable_weights)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "      self.total_loss.update_state(Total_loss)\n",
        "      self.reconstruction_loss.update_state(Reconstruction_loss)\n",
        "      self.kl_loss.update_state(KL_loss)\n",
        "      return { \"Loss\": self.total_loss.result(), \"Reconstruction_loss\": self.reconstruction_loss.result(), \"KL_loss\": self.kl_loss.result(),}\n"
      ],
      "metadata": {
        "id": "AP8qkts851zH",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ML Classifiers\n",
        "class classifiers:\n",
        "\n",
        "  def __init__(self, baseline_results, X_train, y_train, X_dev, y_dev,) -> None:\n",
        "     self.baseline_results = baseline_results\n",
        "     self.X_train = X_train\n",
        "     self.y_train = y_train\n",
        "     self.X_dev = X_dev\n",
        "     self.y_dev = y_dev\n",
        "\n",
        "  def naive_bayes_classification(self, suffix, verbose):\n",
        "    beg = time.time()\n",
        "    clf = GaussianNB()\n",
        "    clf.fit(self.X_train, self.y_train)\n",
        "    y_pred = clf.predict(self.X_dev)\n",
        "    end = time.time()\n",
        "    self.baseline_results[f'Naive-Bayes {suffix}'] = classification_report(self.y_dev, y_pred, output_dict=True, zero_division=1)\n",
        "    self.baseline_results[f'Naive-Bayes {suffix}']['infer_time'] = end-beg\n",
        "    if verbose:\n",
        "        print(f'Naive Bayes\\t{clf.score(self.X_dev, self.y_dev)}')\n",
        "\n",
        "\n",
        "  def knn_classification(self, n_neighbors, suffix, verbose):\n",
        "    beg = time.time()\n",
        "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
        "    clf.fit(self.X_train, self.y_train)\n",
        "    y_pred = clf.predict(self.X_dev)\n",
        "    end = time.time()\n",
        "    self.baseline_results[f'KNN-{n_neighbors} {suffix}'] = classification_report(self.y_dev, y_pred, output_dict=True, zero_division=1)\n",
        "    self.baseline_results[f'KNN-{n_neighbors} {suffix}']['infer_time'] = end-beg\n",
        "    if verbose:\n",
        "        print(f\"KNN with n_neighbors: {n_neighbors}\", end='\\t')\n",
        "        print(clf.score(self.X_dev, self.y_dev))\n",
        "\n",
        "\n",
        "  def random_forest_classification(self, n_estimators, suffix, verbose):\n",
        "    beg = time.time()\n",
        "    clf = RandomForestClassifier(n_estimators=n_estimators)\n",
        "    clf.fit(self.X_train, self.y_train)\n",
        "    y_pred = clf.predict(self.X_dev)\n",
        "    end = time.time()\n",
        "    self.baseline_results[f'RandomForest-{n_estimators} {suffix}'] = classification_report(self.y_dev, y_pred, output_dict=True, zero_division=1)\n",
        "    self.baseline_results[f'RandomForest-{n_estimators} {suffix}']['infer_time'] = end-beg\n",
        "    if verbose:\n",
        "        print(f\"Random Forest with n_estimators: {n_estimators}\", end='\\t')\n",
        "        print(clf.score(self.X_dev, self.y_dev))\n",
        "\n",
        "\n",
        "  def mlp_classification(self, layers, suffix, verbose):\n",
        "    beg = time.time()\n",
        "    clf = MLPClassifier(hidden_layer_sizes=layers, max_iter=3000)\n",
        "    clf.fit(self.X_train, self.y_train)\n",
        "    y_pred = clf.predict(self.X_dev)\n",
        "    end = time.time()\n",
        "    self.baseline_results[f'MLP-{layers} {suffix}'] = classification_report(self.y_dev, y_pred, output_dict=True, zero_division=1)\n",
        "    self.baseline_results[f'MLP-{layers} {suffix}']['infer_time'] = end-beg\n",
        "    if verbose:\n",
        "        print(f\"MLP with layers: {layers}\", end='\\t')\n",
        "        print(clf.score(self.X_dev, self.y_dev))\n",
        "\n",
        "\n",
        "  def svm_classification(self, kernels, suffix, verbose):\n",
        "    beg = time.time()\n",
        "    clf = SVC(kernel=kernels)\n",
        "    clf.fit(self.X_train, self.y_train)\n",
        "    y_pred = clf.predict(self.X_dev)\n",
        "    end = time.time()\n",
        "    self.baseline_results[f'SVM-{kernels} {suffix}'] = classification_report(self.y_dev, y_pred, output_dict=True, zero_division=1)\n",
        "    self.baseline_results[f'SVM-{kernels} {suffix}']['infer_time'] = end-beg\n",
        "    if verbose:\n",
        "        print(f\"SVM with kernel: {kernels}\", end='\\t')\n",
        "        print(clf.score(self.X_dev, self.y_dev))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "sraYNvTgXxdS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Methods and Classes*** ðŸ‘†\n",
        "---\n",
        "# ***Main*** ðŸ‘‡"
      ],
      "metadata": {
        "id": "iNmn807kEt7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset and set the hyperparams\n",
        "Train = pd.read_pickle(\"/content/train_embeddings.p\")\n",
        "Test = pd.read_pickle(\"/content/dev_embeddings_originals.p\")\n",
        "input_dim = Train.shape[1]\n",
        "hidden_dim = 128\n",
        "latent_dim = 50\n",
        "epochs = 30\n",
        "\n",
        "# Build and Train the model\n",
        "vae = VAE(input_dim, hidden_dim, latent_dim)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
        "# vae.fit(Train, epochs=30, batch_size=16)\n",
        "trainset = tf.data.Dataset.from_tensor_slices(Train).batch(16)\n",
        "for epoch in range(epochs):\n",
        "  beta = (epoch/10**8)\n",
        "  total_loss, reconstruction_loss, kl_loss = [], [], []\n",
        "  for batch in trainset:\n",
        "    Losses = vae.train_step(batch, beta)\n",
        "    total_loss.append(Losses['Loss'].numpy())\n",
        "    reconstruction_loss.append( Losses['Reconstruction_loss'].numpy())\n",
        "    kl_loss.append(Losses['KL_loss'].numpy())\n",
        "  print(f\"\\x1b[1mEpoch {epoch+1}/{epochs}\\x1b[0m ==> \\x1b[0;31mLoss\\x1b[0m: {tf.reduce_mean(total_loss)} \\t \\x1b[0;31mReconstruction_Loss\\x1b[0m: {tf.reduce_mean(reconstruction_loss)} \\t \\x1b[0;31mKL_Loss\\x1b[0m: {tf.reduce_mean(kl_loss)}\")"
      ],
      "metadata": {
        "id": "dxBwMA8x9k2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2654c1d4-912f-4c13-cae3-f3be58dd6917"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 384)]        0           []                               \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 128)          49280       ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " z_mean (Dense)                 (None, 50)           6450        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " z_log_var (Dense)              (None, 50)           6450        ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            " sampling_1 (Sampling)          (None, 50)           0           ['z_mean[0][0]',                 \n",
            "                                                                  'z_log_var[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 62,180\n",
            "Trainable params: 62,180\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 50)]              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               6528      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 384)               49536     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 56,064\n",
            "Trainable params: 56,064\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "\u001b[1mEpoch 1/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.04156935214996338 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.04156935214996338 \t \u001b[0;31mKL_Loss\u001b[0m: 145.5533447265625\n",
            "\u001b[1mEpoch 2/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.03545983508229256 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.03545556962490082 \t \u001b[0;31mKL_Loss\u001b[0m: 660.437744140625\n",
            "\u001b[1mEpoch 3/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.03403240442276001 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.03401636332273483 \t \u001b[0;31mKL_Loss\u001b[0m: 1276.90869140625\n",
            "\u001b[1mEpoch 4/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.033126793801784515 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.033095307648181915 \t \u001b[0;31mKL_Loss\u001b[0m: 1714.8609619140625\n",
            "\u001b[1mEpoch 5/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.03251644968986511 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.03246920183300972 \t \u001b[0;31mKL_Loss\u001b[0m: 1987.75\n",
            "\u001b[1mEpoch 6/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.03207335248589516 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.03201156109571457 \t \u001b[0;31mKL_Loss\u001b[0m: 2143.06298828125\n",
            "\u001b[1mEpoch 7/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.031733911484479904 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.031658854335546494 \t \u001b[0;31mKL_Loss\u001b[0m: 2228.758544921875\n",
            "\u001b[1mEpoch 8/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.03146464005112648 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.03137727826833725 \t \u001b[0;31mKL_Loss\u001b[0m: 2275.0751953125\n",
            "\u001b[1mEpoch 9/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.031243421137332916 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.031144537031650543 \t \u001b[0;31mKL_Loss\u001b[0m: 2298.178955078125\n",
            "\u001b[1mEpoch 10/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.031058665364980698 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.0309489443898201 \t \u001b[0;31mKL_Loss\u001b[0m: 2306.4404296875\n",
            "\u001b[1mEpoch 11/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.030903777107596397 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.030783740803599358 \t \u001b[0;31mKL_Loss\u001b[0m: 2305.296142578125\n",
            "\u001b[1mEpoch 12/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.03076857514679432 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.03063858486711979 \t \u001b[0;31mKL_Loss\u001b[0m: 2298.9072265625\n",
            "\u001b[1mEpoch 13/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.0306507870554924 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.030511273071169853 \t \u001b[0;31mKL_Loss\u001b[0m: 2288.402099609375\n",
            "\u001b[1mEpoch 14/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.030548598617315292 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.0303998664021492 \t \u001b[0;31mKL_Loss\u001b[0m: 2275.283203125\n",
            "\u001b[1mEpoch 15/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.03045927919447422 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.0303016509860754 \t \u001b[0;31mKL_Loss\u001b[0m: 2260.2666015625\n",
            "\u001b[1mEpoch 16/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.030380835756659508 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.0302145816385746 \t \u001b[0;31mKL_Loss\u001b[0m: 2244.094482421875\n",
            "\u001b[1mEpoch 17/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.030311372131109238 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.030136773362755775 \t \u001b[0;31mKL_Loss\u001b[0m: 2226.864013671875\n",
            "\u001b[1mEpoch 18/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.030249381437897682 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.03006661869585514 \t \u001b[0;31mKL_Loss\u001b[0m: 2209.492431640625\n",
            "\u001b[1mEpoch 19/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.030194232240319252 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.03000347875058651 \t \u001b[0;31mKL_Loss\u001b[0m: 2192.19189453125\n",
            "\u001b[1mEpoch 20/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.03014468215405941 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.029946161434054375 \t \u001b[0;31mKL_Loss\u001b[0m: 2174.570556640625\n",
            "\u001b[1mEpoch 21/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.030099743977189064 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.02989364042878151 \t \u001b[0;31mKL_Loss\u001b[0m: 2157.026611328125\n",
            "\u001b[1mEpoch 22/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.030059242621064186 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.029845722019672394 \t \u001b[0;31mKL_Loss\u001b[0m: 2139.673095703125\n",
            "\u001b[1mEpoch 23/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.030022524297237396 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.02980172447860241 \t \u001b[0;31mKL_Loss\u001b[0m: 2122.59765625\n",
            "\u001b[1mEpoch 24/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.02998856082558632 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.029760565608739853 \t \u001b[0;31mKL_Loss\u001b[0m: 2105.966064453125\n",
            "\u001b[1mEpoch 25/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.02995692938566208 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.0297218207269907 \t \u001b[0;31mKL_Loss\u001b[0m: 2089.67529296875\n",
            "\u001b[1mEpoch 26/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.029927734285593033 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.029685702174901962 \t \u001b[0;31mKL_Loss\u001b[0m: 2073.58984375\n",
            "\u001b[1mEpoch 27/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.029899703338742256 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.02965087629854679 \t \u001b[0;31mKL_Loss\u001b[0m: 2057.896240234375\n",
            "\u001b[1mEpoch 28/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.029872361570596695 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.02961675263941288 \t \u001b[0;31mKL_Loss\u001b[0m: 2042.72509765625\n",
            "\u001b[1mEpoch 29/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.02984645776450634 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.0295842494815588 \t \u001b[0;31mKL_Loss\u001b[0m: 2027.772705078125\n",
            "\u001b[1mEpoch 30/30\u001b[0m ==> \u001b[0;31mLoss\u001b[0m: 0.029821649193763733 \t \u001b[0;31mReconstruction_Loss\u001b[0m: 0.029552912339568138 \t \u001b[0;31mKL_Loss\u001b[0m: 2013.2288818359375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Trained encoder to get the compressed vectors\n",
        "trained_encoder = vae.encoder\n",
        "trained_encoder.trainable = False  # freeze the weights\n",
        "# Trainset\n",
        "z_mean, z_log_var, z = trained_encoder.predict(Train, verbose=0)\n",
        "print(f'Size of the trainset before {Train.nbytes} and after {z.nbytes} dimensionality reduction.')\n",
        "# Testset\n",
        "z_mean_test, z_log_var_test, z_test = trained_encoder.predict(Test, verbose=0)\n",
        "# Result\n",
        "baseline_results = {}\n",
        "# Lables\n",
        "# df = pd.read_json('train.json', orient='index') # AspectSentiment\n",
        "# labels = df['polarity'].values\n",
        "# df_test = pd.read_json('test.json', orient='index')\n",
        "# labels_test = df_test['polarity'].values\n",
        "df = pd.read_table('train_ml.tsv') # CheckThatLab\n",
        "labels = df['label'].values\n",
        "df_test = pd.read_table('dev_ml.tsv')\n",
        "labels_test = df_test['label'].values\n",
        "# df = pd.read_csv('Train.csv') # FEVER\n",
        "# labels = df['label'].values\n",
        "# df_test = pd.read_csv('Test.csv')\n",
        "# labels_test = df_test['label'].values\n",
        "# Classifiers\n",
        "suffix = 'VAE_' + 'CheckThatLab'\n",
        "cls = classifiers(baseline_results, X_train=z, y_train=labels, X_dev=z_test, y_dev=labels_test)\n",
        "cls.naive_bayes_classification(suffix, True)\n",
        "cls.knn_classification(50, suffix, True)\n",
        "cls.random_forest_classification(100, suffix, True)\n",
        "cls.mlp_classification((200, 100), suffix, True)\n",
        "cls.svm_classification('rbf', suffix, True)"
      ],
      "metadata": {
        "id": "X8m-OUtSEaTg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4f115a-ad78-480f-d882-3dc40a59be66"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of the trainset before 10180608 and after 1325600 dimensionality reduction.\n",
            "Naive Bayes\t0.655\n",
            "KNN with n_neighbors: 50\t0.6483333333333333\n",
            "Random Forest with n_estimators: 100\t0.745\n",
            "MLP with layers: (200, 100)\t0.8\n",
            "SVM with kernel: rbf\t0.7433333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(f'{suffix}.pickle', 'wb') as f:\n",
        "    pickle.dump(baseline_results, f, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "1rjgHHZmFxdO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}